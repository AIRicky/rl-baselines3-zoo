Pendulum-v0:
  n_timesteps: 20000
  policy: 'MlpPolicy'
  learning_rate: !!float 1e-3
  use_sde: True
  n_episodes_rollout: 1
  gradient_steps: -1
  train_freq: -1
  policy_kwargs: "dict(log_std_init=-2, net_arch=[64, 64])"

LunarLanderContinuous-v2:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  batch_size: 256
  learning_starts: 1000

# Tuned
BipedalWalker-v3:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 300000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.02
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300])"

# Almost tuned
BipedalWalkerHardcore-v3:
  env_wrapper:
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  n_timesteps: !!float 3e7
  policy: 'MlpPolicy'
  learning_rate: lin_7.3e-4
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.01
  train_freq: 64
  gradient_steps: 64
  learning_starts: 10000
  use_sde: True
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300], use_expln=True)"

# === Bullet envs ===

HalfCheetahBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 100
  policy_kwargs: "dict(net_arch=[64, 64])"

# Tuned
AntBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 100
  policy_kwargs: "dict(net_arch=[64, 64])"

HopperBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 100
  policy_kwargs: "dict(net_arch=[64, 64])"

Walker2DBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 100
  policy_kwargs: "dict(net_arch=[64, 64])"

ReacherBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 100
  policy_kwargs: "dict(net_arch=[64, 64])"

donkey-generated-track-v0:
  env_wrapper:
    - gym.wrappers.time_limit.TimeLimit:
        max_episode_steps: 700
    - utils.wrappers.HistoryWrapper:
        horizon: 5
    - utils.wrappers.TimeFeatureWrapper
    - stable_baselines3.common.monitor.Monitor:
        filename: None
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  std_init: 0.1
  n_steps: 700
  policy_kwargs: "dict(net_arch=[256, 256])"
  best_individual: "logs/sac/donkey-generated-track-v0_114/rl_model_400000_steps.zip"
